{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Correcting the file path\n",
    "file_path = r'C:\\Graduate Admission Prediction using ANN\\Admission_Predict_Ver1.1.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displays the first few rows of the dataset to understand its structure.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape   #Provides the dimensions of the DataFrame (number of rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()   #for checking any missing values in columns and it is also Gives a concise summary of the DataFrame including the non-null count and data type of each column, useful for identifying missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()  #for checking any duplicated row is present in databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Serial No.'],inplace=True)     #Removes the 'Serial No.' column as it's likely just an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line selects all rows (:) and all columns except the last one (0:-1) from the DataFrame data.\n",
    "#This means X will contain all the feature columns that are used to input into the model.\n",
    "#The slicing 0:-1 uses Python's list slicing syntax where -1 represents the last item, so 0:-1 includes everything up to but not including the last column.\n",
    "\n",
    "X= data.iloc[:,0:-1]\n",
    "\n",
    "\n",
    "\n",
    "#This line selects all rows (:) and only the last column (-1) of the DataFrame.\n",
    "#This column in y will be the target variable or the output that your model aims to predict.\n",
    "#The -1 specifically selects the last column, which is typically reserved for the label or dependent variable in many data science workflows where the dataset is structured with features first and the label last.\n",
    "y= data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "495        332          108                  5  4.5   4.0  9.02         1\n",
       "496        337          117                  5  5.0   5.0  9.87         1\n",
       "497        330          120                  5  4.5   5.0  9.56         1\n",
       "498        312          103                  4  4.0   5.0  8.43         0\n",
       "499        327          113                  4  4.5   4.5  9.04         0\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the data into training and test sets, with 20% of the data reserved for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>310</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>300</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>322</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "238        310          104                  3  2.0   3.5  8.37         0\n",
       "438        318          110                  1  2.5   3.5  8.54         1\n",
       "475        300          101                  3  3.5   2.5  7.88         0\n",
       "58         300           99                  1  3.0   2.0  6.80         1\n",
       "380        322          104                  3  3.5   4.0  8.84         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler    #Scales the feature data to a range [0, 1], which is often beneficial for neural network convergence.\n",
    "scaler  = MinMaxScaler()\n",
    "\n",
    "\n",
    "#fit_transform() on X_train and transform() on X_test: Applies scaling. It’s important to fit the scaler only on training data to avoid leakage of information from the test set.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4       , 0.42857143, 0.5       , ..., 0.57142857, 0.50320513,\n",
       "        0.        ],\n",
       "       [0.56      , 0.64285714, 0.        , ..., 0.57142857, 0.55769231,\n",
       "        1.        ],\n",
       "       [0.2       , 0.32142857, 0.5       , ..., 0.28571429, 0.34615385,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.74038462,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.77884615,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.32051282,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential    #Sequential: This is imported from Keras and is used to create a model that is a linear stack of layers.\n",
    "from keras.layers import Dense   #Dense: This is also imported from Keras and is used to create densely-connected (fully connected) neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Defining the model:\n",
    "model = Sequential()\n",
    "\n",
    "#Adding layers to the model:\n",
    "model.add(Dense(7, activation='relu', input_shape=(7,)))  #The first layer in the model. It is a densely-connected layer with 7 neurons. The activation='relu' specifies that the rectified linear unit function should be used as the activation function. input_shape=(7,) defines the shape of the input data (7 features), indicating that this is the first layer.\n",
    "model.add(Dense(7, activation='relu'))   #A second densely-connected layer with 7 neurons, also using the ReLU activation function. Since it's not the first layer in the network, there is no need to specify input_shape as Keras automatically infers the shape from the previous layer.\n",
    "model.add(Dense(1,activation='linear'))  #The output layer of the model with a single neuron. This uses a linear activation function. In the context of regression, a linear activation function allows the model to predict continuous values directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Model\n",
    "model.compile(loss='mean_squared_error',optimizer='Adam')  #Configures the model with the Adam optimizer and mean squared error loss function, which is standard for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.0326 - val_loss: 0.8882\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7200 - val_loss: 0.6811\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5911 - val_loss: 0.5194\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4503 - val_loss: 0.3976\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3439 - val_loss: 0.3012\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2442 - val_loss: 0.2214\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1809 - val_loss: 0.1543\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1299 - val_loss: 0.1016\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0846 - val_loss: 0.0638\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0519 - val_loss: 0.0396\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0308 - val_loss: 0.0260\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0205 - val_loss: 0.0191\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0144 - val_loss: 0.0160\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0124 - val_loss: 0.0143\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0107 - val_loss: 0.0131\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0083 - val_loss: 0.0112\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0089 - val_loss: 0.0104\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0071 - val_loss: 0.0097\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0061 - val_loss: 0.0075\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0054 - val_loss: 0.0072\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0050 - val_loss: 0.0069\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0054 - val_loss: 0.0067\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0047 - val_loss: 0.0065\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0064\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0051 - val_loss: 0.0062\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.0059\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0046 - val_loss: 0.0058\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0057\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0052\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0050\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0050\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0050\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0049\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0049\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0048\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0048\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0047\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0047\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0047\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0047\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0046\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0034 - val_loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "#Train Model\n",
    "history = model.fit(X_train_scaled,y_train,epochs=100,validation_split=0.2)  #Trains the model for 100 epochs, using 20% of the training data as validation data to monitor performance on unseen data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)   #Predicts the output for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7976522958925928"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates the R² score, which provides an indication of goodness of fit and the percentage of variance explained by the model.\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a692c68560>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize Training Progress\n",
    "#Plots: Shows how the loss and validation loss decrease over epochs, which can indicate whether the model is learning effectively and if it's overfitting or underfitting.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
